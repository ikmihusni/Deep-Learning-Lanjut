{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPeuDDLPAhfwf/20w2V9N96"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7LvoqXFX56K0","executionInfo":{"status":"ok","timestamp":1768821297868,"user_tz":-420,"elapsed":19825,"user":{"displayName":"Husni Mubarok","userId":"08248473095387935138"}},"outputId":"5d482b47-6dba-4424-b2e1-649dad13c029"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow==2.19.0 in /usr/local/lib/python3.12/dist-packages (2.19.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.12.19)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.7.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (5.29.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.32.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (4.15.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.0.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.76.0)\n","Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.19.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.10.0)\n","Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.0.2)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.15.1)\n","Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.5.4)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow==2.19.0) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.18.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (2026.1.4)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.10)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.1.5)\n","Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19.0) (3.0.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19.0) (0.1.2)\n","TensorFlow version: 2.19.0\n","GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"]}],"source":["!pip install tensorflow==2.19.0\n","import tensorflow as tf\n","print(\"TensorFlow version:\", tf.__version__)\n","print(\"GPU available:\",\n","tf.config.list_physical_devices('GPU'))"]},{"cell_type":"code","source":["import requests\n","import numpy as np\n","# Unduh teks Shakespeare\n","url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n","text = requests.get(url).text\n","print(f\"Panjang teks: {len(text)} karakter\")\n","print(\"Contoh:\\n\", text[:500])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r0nZ-_PD6R4J","executionInfo":{"status":"ok","timestamp":1768821298091,"user_tz":-420,"elapsed":221,"user":{"displayName":"Husni Mubarok","userId":"08248473095387935138"}},"outputId":"bbc9ed2f-6abc-470d-d4f9-b692cdd2e497"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Panjang teks: 1115394 karakter\n","Contoh:\n"," First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n","All:\n","We know't, we know't.\n","\n","First Citizen:\n","Let us kill him, and we'll have corn at our own price.\n","Is't a verdict?\n","\n","All:\n","No more talking on't; let it be done: away, away!\n","\n","Second Citizen:\n","One word, good citizens.\n","\n","First Citizen:\n","We are accounted poor\n"]}]},{"cell_type":"code","source":["# Bangun vocabulary\n","vocab = sorted(set(text))\n","vocab_size = len(vocab)\n","print(f\"Vocabulary size: {vocab_size}\")\n","# Mapping char â†” index\n","char_to_idx = {ch: i for i, ch in enumerate(vocab)}\n","idx_to_char = np.array(vocab)\n","# Encode teks menjadi integer\n","text_as_int = np.array([char_to_idx[c] for c in text])\n","# Parameter\n","seq_length = 100 # Panjang konteks\n","batch_size = 64\n","buffer_size = 10000\n","\n","# Buat dataset pasangan (input, target)\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n","def split_input_target(chunk):\n","    input_text = chunk[:-1]\n","    target_text = chunk[1:]\n","    return input_text, target_text\n","dataset = sequences.map(split_input_target)\n","dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n","dataset = dataset.prefetch(tf.data.AUTOTUNE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K_CcIdV36eYg","executionInfo":{"status":"ok","timestamp":1768821298633,"user_tz":-420,"elapsed":533,"user":{"displayName":"Husni Mubarok","userId":"08248473095387935138"}},"outputId":"f5b7d4cd-e03b-456e-92ed-626502fc15bf"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size: 65\n"]}]},{"cell_type":"code","source":["class PositionalEmbedding(tf.keras.layers.Layer):\n","    def __init__(self, vocab_size, d_model, max_len=5000):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n","        self.pos_encoding = self.positional_encoding(max_len, d_model)\n","\n","    def positional_encoding(self, position, d_model):\n","        # Learned positional embedding (lebih sederhana)\n","        return tf.Variable(tf.random.normal((1, position, d_model)))\n","\n","    def call(self, x):\n","        seq_len = tf.shape(x)[1]\n","        x = self.embedding(x) * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x = x + self.pos_encoding[:, :seq_len, :]\n","        return x"],"metadata":{"id":"LUyk8yN96qom","executionInfo":{"status":"ok","timestamp":1768821298637,"user_tz":-420,"elapsed":1,"user":{"displayName":"Husni Mubarok","userId":"08248473095387935138"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class CausalSelfAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","        assert d_model % num_heads == 0\n","        self.depth = d_model // num_heads\n","        self.wq = tf.keras.layers.Dense(d_model)\n","        self.wk = tf.keras.layers.Dense(d_model)\n","        self.wv = tf.keras.layers.Dense(d_model)\n","        self.dense = tf.keras.layers.Dense(d_model)\n","\n","    def split_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def call(self, v, k, q, mask=None):\n","        batch_size = tf.shape(q)[0]\n","        q = self.wq(q)\n","        k = self.wk(k)\n","        v = self.wv(v)\n","        q = self.split_heads(q, batch_size)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","        # Skala dot-product attention\n","        matmul_qk = tf.matmul(q, k, transpose_b=True)\n","        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","        # Causal mask\n","        if mask is None:\n","            seq_len = tf.shape(q)[2]\n","            mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","            mask = mask[tf.newaxis, tf.newaxis, :, :]\n","        scaled_attention_logits += (mask * -1e9)\n","        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n","        output = tf.matmul(attention_weights, v)\n","        output = tf.transpose(output, perm=[0, 2, 1, 3])\n","        output = tf.reshape(output, (batch_size, -1, self.d_model))\n","        return self.dense(output)"],"metadata":{"id":"5qi3ggQe6vXa","executionInfo":{"status":"ok","timestamp":1768821298640,"user_tz":-420,"elapsed":1,"user":{"displayName":"Husni Mubarok","userId":"08248473095387935138"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["    def call(self, v, k, q, mask=None):\n","        batch_size = tf.shape(q)[0]\n","        q = self.wq(q)\n","        k = self.wk(k)\n","        v = self.wv(v)\n","        q = self.split_heads(q, batch_size)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","        # Skala dot-product attention\n","        matmul_qk = tf.matmul(q, k, transpose_b=True)\n","        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","        # Causal mask\n","        if mask is None:\n","            seq_len = tf.shape(q)[2]\n","            mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","            mask = mask[tf.newaxis, tf.newaxis, :, :]\n","        scaled_attention_logits += (mask * -1e9)\n","        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n","        output = tf.matmul(attention_weights, v)\n","        output = tf.transpose(output, perm=[0, 2, 1, 3])\n","        output = tf.reshape(output, (batch_size, -1, self.d_model))\n","        return self.dense(output)"],"metadata":{"id":"OtxcW5p96zPx","executionInfo":{"status":"ok","timestamp":1768821298642,"user_tz":-420,"elapsed":1,"user":{"displayName":"Husni Mubarok","userId":"08248473095387935138"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def point_wise_feed_forward_network(d_model, dff):\n","    return tf.keras.Sequential([\n","        tf.keras.layers.Dense(dff, activation='relu'),\n","        tf.keras.layers.Dense(d_model)\n","    ])"],"metadata":{"id":"35ECznN963c-","executionInfo":{"status":"ok","timestamp":1768821298653,"user_tz":-420,"elapsed":2,"user":{"displayName":"Husni Mubarok","userId":"08248473095387935138"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class DecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super().__init__()\n","        self.mha = CausalSelfAttention(d_model, num_heads)\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, x, training):\n","        attn_output = self.mha(x, x, x) # (batch_size, input_seq_len, d_model)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(x + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        out2 = self.layernorm2(out1 + ffn_output)\n","        return out2"],"metadata":{"id":"c9n5FuNt7E9x","executionInfo":{"status":"ok","timestamp":1768821298787,"user_tz":-420,"elapsed":105,"user":{"displayName":"Husni Mubarok","userId":"08248473095387935138"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class GPT(tf.keras.Model):\n","    def __init__(self, vocab_size, d_model, num_layers, num_heads, dff, max_len=1000, rate=0.1):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.pos_embedding = PositionalEmbedding(vocab_size, d_model, max_len)\n","        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","        self.final_layer = tf.keras.layers.Dense(vocab_size)\n","\n","    def call(self, x, training):\n","        x = self.pos_embedding(x) # (batch, seq_len, d_model)\n","        x = self.dropout(x, training=training)\n","        for i in range(self.num_layers):\n","            x = self.dec_layers[i](x, training=training)\n","        logits = self.final_layer(x) # (batch, seq_len, vocab_size)\n","        return logits"],"metadata":{"id":"2TCC7HYa7uMR","executionInfo":{"status":"ok","timestamp":1768821298788,"user_tz":-420,"elapsed":4,"user":{"displayName":"Husni Mubarok","userId":"08248473095387935138"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Hyperparameter\n","vocab_size = len(vocab)\n","d_model = 128\n","num_layers = 4\n","um_heads = 8\n","dff = 512\n","max_len = 512 # Increased max_len to accommodate longer generated sequences (e.g., 6 + 300 = 306)\n","# Inisialisasi model\n","model = GPT(vocab_size, d_model, num_layers, num_heads, dff, max_len)\n","# Optimizer dan loss\n","optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","def loss_function(real, pred):\n","# Opsional: mask padding jika ada (di dataset ini, tidak ada padding sebenarnya)\n","# Tapi kita pertahankan untuk umum\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","    return tf.reduce_mean(loss_)\n","@tf.function\n","def train_step(inp, tar):\n","    with tf.GradientTape() as tape:\n","        predictions = model(inp, training=True)\n","        loss = loss_function(tar, predictions)\n","    gradients = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","    return loss # Kembalikan loss untuk logging\n","\n","# Pelatihan\n","EPOCHS = 50\n","for epoch in range(EPOCHS):\n","    epoch_loss = 0.0\n","    num_batches = 0\n","    for batch, (inp, tar) in enumerate(dataset):\n","        loss = train_step(inp, tar)\n","        epoch_loss += loss\n","        num_batches += 1\n","        if batch % 100 == 0:\n","            avg_loss = epoch_loss / num_batches\n","            print(f'Epoch {epoch+1} Batch {batch} Avg Loss {avg_loss:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ezcrVtfD7uBV","executionInfo":{"status":"ok","timestamp":1768822289546,"user_tz":-420,"elapsed":429111,"user":{"displayName":"Husni Mubarok","userId":"08248473095387935138"}},"outputId":"13f6ca6f-87fa-4b50-dd63-80749963ea96"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 Batch 0 Avg Loss 4.3424\n","Epoch 1 Batch 100 Avg Loss 3.0122\n","Epoch 2 Batch 0 Avg Loss 2.3697\n","Epoch 2 Batch 100 Avg Loss 2.3185\n","Epoch 3 Batch 0 Avg Loss 2.1703\n","Epoch 3 Batch 100 Avg Loss 2.1082\n","Epoch 4 Batch 0 Avg Loss 1.9953\n","Epoch 4 Batch 100 Avg Loss 1.9536\n","Epoch 5 Batch 0 Avg Loss 1.8618\n","Epoch 5 Batch 100 Avg Loss 1.8432\n","Epoch 6 Batch 0 Avg Loss 1.7827\n","Epoch 6 Batch 100 Avg Loss 1.7604\n","Epoch 7 Batch 0 Avg Loss 1.7434\n","Epoch 7 Batch 100 Avg Loss 1.6958\n","Epoch 8 Batch 0 Avg Loss 1.6333\n","Epoch 8 Batch 100 Avg Loss 1.6480\n","Epoch 9 Batch 0 Avg Loss 1.6365\n","Epoch 9 Batch 100 Avg Loss 1.6120\n","Epoch 10 Batch 0 Avg Loss 1.5501\n","Epoch 10 Batch 100 Avg Loss 1.5799\n","Epoch 11 Batch 0 Avg Loss 1.6062\n","Epoch 11 Batch 100 Avg Loss 1.5562\n","Epoch 12 Batch 0 Avg Loss 1.5635\n","Epoch 12 Batch 100 Avg Loss 1.5333\n","Epoch 13 Batch 0 Avg Loss 1.4904\n","Epoch 13 Batch 100 Avg Loss 1.5127\n","Epoch 14 Batch 0 Avg Loss 1.4881\n","Epoch 14 Batch 100 Avg Loss 1.4965\n","Epoch 15 Batch 0 Avg Loss 1.5245\n","Epoch 15 Batch 100 Avg Loss 1.4825\n","Epoch 16 Batch 0 Avg Loss 1.4484\n","Epoch 16 Batch 100 Avg Loss 1.4656\n","Epoch 17 Batch 0 Avg Loss 1.4167\n","Epoch 17 Batch 100 Avg Loss 1.4539\n","Epoch 18 Batch 0 Avg Loss 1.4584\n","Epoch 18 Batch 100 Avg Loss 1.4433\n","Epoch 19 Batch 0 Avg Loss 1.3304\n","Epoch 19 Batch 100 Avg Loss 1.4337\n","Epoch 20 Batch 0 Avg Loss 1.4011\n","Epoch 20 Batch 100 Avg Loss 1.4269\n","Epoch 21 Batch 0 Avg Loss 1.3965\n","Epoch 21 Batch 100 Avg Loss 1.4167\n","Epoch 22 Batch 0 Avg Loss 1.3910\n","Epoch 22 Batch 100 Avg Loss 1.4075\n","Epoch 23 Batch 0 Avg Loss 1.3811\n","Epoch 23 Batch 100 Avg Loss 1.4015\n","Epoch 24 Batch 0 Avg Loss 1.3541\n","Epoch 24 Batch 100 Avg Loss 1.3935\n","Epoch 25 Batch 0 Avg Loss 1.3975\n","Epoch 25 Batch 100 Avg Loss 1.3863\n","Epoch 26 Batch 0 Avg Loss 1.3778\n","Epoch 26 Batch 100 Avg Loss 1.3832\n","Epoch 27 Batch 0 Avg Loss 1.3928\n","Epoch 27 Batch 100 Avg Loss 1.3766\n","Epoch 28 Batch 0 Avg Loss 1.3679\n","Epoch 28 Batch 100 Avg Loss 1.3739\n","Epoch 29 Batch 0 Avg Loss 1.3311\n","Epoch 29 Batch 100 Avg Loss 1.3661\n","Epoch 30 Batch 0 Avg Loss 1.3711\n","Epoch 30 Batch 100 Avg Loss 1.3607\n","Epoch 31 Batch 0 Avg Loss 1.3530\n","Epoch 31 Batch 100 Avg Loss 1.3588\n","Epoch 32 Batch 0 Avg Loss 1.3499\n","Epoch 32 Batch 100 Avg Loss 1.3524\n","Epoch 33 Batch 0 Avg Loss 1.3372\n","Epoch 33 Batch 100 Avg Loss 1.3463\n","Epoch 34 Batch 0 Avg Loss 1.3400\n","Epoch 34 Batch 100 Avg Loss 1.3424\n","Epoch 35 Batch 0 Avg Loss 1.3329\n","Epoch 35 Batch 100 Avg Loss 1.3397\n","Epoch 36 Batch 0 Avg Loss 1.2514\n","Epoch 36 Batch 100 Avg Loss 1.3379\n","Epoch 37 Batch 0 Avg Loss 1.3473\n","Epoch 37 Batch 100 Avg Loss 1.3325\n","Epoch 38 Batch 0 Avg Loss 1.3348\n","Epoch 38 Batch 100 Avg Loss 1.3301\n","Epoch 39 Batch 0 Avg Loss 1.3046\n","Epoch 39 Batch 100 Avg Loss 1.3277\n","Epoch 40 Batch 0 Avg Loss 1.3102\n","Epoch 40 Batch 100 Avg Loss 1.3249\n","Epoch 41 Batch 0 Avg Loss 1.2886\n","Epoch 41 Batch 100 Avg Loss 1.3212\n","Epoch 42 Batch 0 Avg Loss 1.2962\n","Epoch 42 Batch 100 Avg Loss 1.3159\n","Epoch 43 Batch 0 Avg Loss 1.2972\n","Epoch 43 Batch 100 Avg Loss 1.3140\n","Epoch 44 Batch 0 Avg Loss 1.3196\n","Epoch 44 Batch 100 Avg Loss 1.3120\n","Epoch 45 Batch 0 Avg Loss 1.3456\n","Epoch 45 Batch 100 Avg Loss 1.3069\n","Epoch 46 Batch 0 Avg Loss 1.2884\n","Epoch 46 Batch 100 Avg Loss 1.3105\n","Epoch 47 Batch 0 Avg Loss 1.2848\n","Epoch 47 Batch 100 Avg Loss 1.3045\n","Epoch 48 Batch 0 Avg Loss 1.2498\n","Epoch 48 Batch 100 Avg Loss 1.3019\n","Epoch 49 Batch 0 Avg Loss 1.2850\n","Epoch 49 Batch 100 Avg Loss 1.3000\n","Epoch 50 Batch 0 Avg Loss 1.2689\n","Epoch 50 Batch 100 Avg Loss 1.2972\n"]}]},{"cell_type":"code","source":["def generate_text(model, start_string, length=100, temperature=1.0):\n","    # Encode string awal menjadi indeks\n","    input_ids = [char_to_idx.get(s, 0) for s in start_string] # gunakan 0 jika karakter tidak dikenal\n","    input_ids = tf.expand_dims(input_ids, 0) # shape: (1, seq_len)\n","    text_generated = []\n","    # Jalankan inference autoregressive\n","    for i in range(length):\n","        # Prediksi distribusi probabilitas untuk seluruh urutan\n","        predictions = model(input_ids, training=False) # shape: (1, seq_len, vocab_size)\n","        # Ambil prediksi untuk posisi terakhir\n","        last_pred = predictions[:, -1, :] # shape: (1, vocab_size)\n","        # Terapkan temperature scaling (opsional, untuk variasi)\n","        last_pred = last_pred / temperature\n","        # Sampling dari distribusi\n","        predicted_id = tf.random.categorical(last_pred, num_samples=1) # shape: (1, 1)\n","        predicted_id = tf.squeeze(predicted_id, axis=-1).numpy()[0] # scalar int\n","        # Tambahkan ke input berikutnya\n","        input_ids = tf.concat([input_ids, [[predicted_id]]], axis=1)\n","        # Tambahkan ke hasil\n","        text_generated.append(idx_to_char[predicted_id])\n","    return start_string + ''.join(text_generated)"],"metadata":{"id":"TMauX2SB7t1u","executionInfo":{"status":"ok","timestamp":1768821784654,"user_tz":-420,"elapsed":3,"user":{"displayName":"Husni Mubarok","userId":"08248473095387935138"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Generate teks\n","generated = generate_text(model, start_string=\"ROMEO:\",\n","length=300, temperature=0.8)\n","print(generated)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XLUM9hAO63NC","executionInfo":{"status":"ok","timestamp":1768822972370,"user_tz":-420,"elapsed":58841,"user":{"displayName":"Husni Mubarok","userId":"08248473095387935138"}},"outputId":"bb038bd4-84ba-4899-b733-ca67bf0a682c"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["ROMEO:\n","Come I do you think, and you well not stay to have perceived\n","Your wells?\n","\n","ESCALUS:\n","Therefore it imor goty, y y oringre.\n","\n","Wind be, matines were y y?\n","May howas mes, g, he ahe he corenoure t ay I ale onourest thles e moure bour me thithede con. ples s; abestsigles t t t gre wes! t t hot,\n","be t t we t b\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"fbQgLYDl625P","executionInfo":{"status":"ok","timestamp":1768821740901,"user_tz":-420,"elapsed":13,"user":{"displayName":"Husni Mubarok","userId":"08248473095387935138"}}},"execution_count":11,"outputs":[]}]}