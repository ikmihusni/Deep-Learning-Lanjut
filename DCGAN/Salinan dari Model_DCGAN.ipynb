{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wIUF6-4qkPvS"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU Terdeteksi:  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"]}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","\n","# Konfigurasi agar GPU digunakan secara efisien\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if len(physical_devices) \u003e 0:\n","    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n","    print(\"GPU Terdeteksi: \", physical_devices[0])\n","else:\n","    print(\"Peringatan: GPU tidak terdeteksi. Training akan lambat.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3bc46c68"},"outputs":[{"name":"stdout","output_type":"stream","text":["mv: cannot stat '/content/kaggle.json': No such file or directory\n","chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n","Kaggle API credentials set up successfully!\n"]}],"source":["# Create the .kaggle directory if it doesn't exist\n","!mkdir -p ~/.kaggle\n","\n","# Move the kaggle.json file to the .kaggle directory\n","# IMPORTANT: Make sure you have uploaded kaggle.json to your Colab session's /content/ directory first.\n","# If you uploaded it somewhere else, adjust the source path accordingly.\n","!mv /content/kaggle.json ~/.kaggle/\n","\n","# Set read-only permissions for the kaggle.json file for security\n","!chmod 600 ~/.kaggle/kaggle.json\n","\n","print(\"Kaggle API credentials set up successfully!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jN7L5yaGlE_k"},"outputs":[{"name":"stdout","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/usr/local/bin/kaggle\", line 10, in \u003cmodule\u003e\n","    sys.exit(main())\n","             ^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/kaggle/cli.py\", line 68, in main\n","    out = args.func(**command_args)\n","          ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 1741, in dataset_download_cli\n","    with self.build_kaggle_client() as kaggle:\n","         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 688, in build_kaggle_client\n","    username=self.config_values['username'],\n","             ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n","KeyError: 'username'\n","unzip:  cannot find or open /content/lego-brick-images.zip, /content/lego-brick-images.zip.zip or /content/lego-brick-images.zip.ZIP.\n","rm: cannot remove '/content/lego-brick-images.zip': No such file or directory\n","Dataset downloaded and extracted to /content/dataset/\n"]}],"source":["# Install kaggle if not already installed\n","!pip install -q kaggle\n","\n","# Make sure your Kaggle API credentials (kaggle.json) are set up in ~/.kaggle/\n","# If you haven't done this, please refer to the previous instructions.\n","\n","# Download the dataset\n","!kaggle datasets download -d joosthazelzet/lego-brick-images\n","\n","# Unzip the downloaded dataset to the specified directory\n","# The zip file will be downloaded to the current working directory (/content/)\n","!unzip -q /content/lego-brick-images.zip -d /content/dataset/\n","\n","# Clean up the zip file after extraction\n","!rm /content/lego-brick-images.zip\n","\n","print(\"Dataset downloaded and extracted to /content/dataset/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"VxBFUAWhkjSr"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset tidak ditemukan. Membuat data dummy untuk demonstrasi struktur kode.\n","Found 100 files.\n"]}],"source":["# Parameter Konfigurasi\n","BATCH_SIZE = 64\n","IMG_HEIGHT = 64  # Menggunakan 64x64 agar detail LEGO cukup terlihat\n","IMG_WIDTH = 64\n","CHANNELS = 3     # RGB\n","\n","\n","# --- OPSI B: Membuat Dataset Loader ---\n","# Ganti 'path/to/images' dengan path folder dataset Anda\n","dataset_dir = '/content/dataset/LEGO brick images v1'\n","\n","# Jika folder kosong/tidak ada, kita buat dummy data agar kode tetap jalan (untuk testing)\n","if not os.path.exists(dataset_dir) or not os.listdir(dataset_dir):\n","    print(\"Dataset tidak ditemukan. Membuat data dummy untuk demonstrasi struktur kode.\")\n","    os.makedirs(dataset_dir, exist_ok=True)\n","    # Membuat 100 gambar random noise sebagai placeholder\n","    for i in range(100):\n","        img = np.random.randint(0, 255, (64, 64, 3), dtype=np.uint8)\n","        tf.keras.utils.save_img(f\"{dataset_dir}/dummy_{i}.png\", img)\n","\n","# Memuat dataset\n","train_dataset = tf.keras.utils.image_dataset_from_directory(\n","    dataset_dir,\n","    label_mode=None, # Kita tidak butuh label untuk GAN\n","    image_size=(IMG_HEIGHT, IMG_WIDTH),\n","    batch_size=BATCH_SIZE\n",")\n","\n","# Normalisasi ke [-1, 1]\n","train_dataset = train_dataset.map(lambda x: (x - 127.5) / 127.5)\n","\n","# Optimasi performa (Caching \u0026 Prefetching)\n","train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"pPqGPEKfm_ak"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","/usr/local/lib/python3.12/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003eModel: \"generator\"\u003c/span\u003e\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[1mModel: \"generator\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u003cspan style=\"font-weight: bold\"\u003e Layer (type)                    \u003c/span\u003e┃\u003cspan style=\"font-weight: bold\"\u003e Output Shape           \u003c/span\u003e┃\u003cspan style=\"font-weight: bold\"\u003e       Param # \u003c/span\u003e┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ dense (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eDense\u003c/span\u003e)                   │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e32768\u003c/span\u003e)          │     \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e4,194,304\u003c/span\u003e │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization             │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e32768\u003c/span\u003e)          │       \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e131,072\u003c/span\u003e │\n","│ (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eBatchNormalization\u003c/span\u003e)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ leaky_re_lu (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eLeakyReLU\u003c/span\u003e)         │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e32768\u003c/span\u003e)          │             \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ reshape (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eReshape\u003c/span\u003e)               │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e8\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e8\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e512\u003c/span\u003e)      │             \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_transpose                │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e16\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e16\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e256\u003c/span\u003e)    │     \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e2,097,152\u003c/span\u003e │\n","│ (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eConv2DTranspose\u003c/span\u003e)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_1           │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e16\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e16\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e256\u003c/span\u003e)    │         \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e1,024\u003c/span\u003e │\n","│ (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eBatchNormalization\u003c/span\u003e)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ leaky_re_lu_1 (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eLeakyReLU\u003c/span\u003e)       │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e16\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e16\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e256\u003c/span\u003e)    │             \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_transpose_1              │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e32\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e32\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e128\u003c/span\u003e)    │       \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e524,288\u003c/span\u003e │\n","│ (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eConv2DTranspose\u003c/span\u003e)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_2           │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e32\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e32\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e128\u003c/span\u003e)    │           \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e512\u003c/span\u003e │\n","│ (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eBatchNormalization\u003c/span\u003e)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ leaky_re_lu_2 (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eLeakyReLU\u003c/span\u003e)       │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e32\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e32\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e128\u003c/span\u003e)    │             \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_transpose_2              │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e64\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e64\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e64\u003c/span\u003e)     │       \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e131,072\u003c/span\u003e │\n","│ (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eConv2DTranspose\u003c/span\u003e)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_3           │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e64\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e64\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e64\u003c/span\u003e)     │           \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e256\u003c/span\u003e │\n","│ (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eBatchNormalization\u003c/span\u003e)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ leaky_re_lu_3 (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eLeakyReLU\u003c/span\u003e)       │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e64\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e64\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e64\u003c/span\u003e)     │             \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eConv2D\u003c/span\u003e)                 │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e64\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e64\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e3\u003c/span\u003e)      │         \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e1,731\u003c/span\u003e │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","\u003c/pre\u003e\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32768\u001b[0m)          │     \u001b[38;5;34m4,194,304\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32768\u001b[0m)          │       \u001b[38;5;34m131,072\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32768\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_transpose                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │     \u001b[38;5;34m2,097,152\u001b[0m │\n","│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │         \u001b[38;5;34m1,024\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_transpose_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m524,288\u001b[0m │\n","│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_transpose_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │       \u001b[38;5;34m131,072\u001b[0m │\n","│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │         \u001b[38;5;34m1,731\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003e Total params: \u003c/span\u003e\u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e7,081,411\u003c/span\u003e (27.01 MB)\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,081,411\u001b[0m (27.01 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003e Trainable params: \u003c/span\u003e\u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e7,014,979\u003c/span\u003e (26.76 MB)\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,014,979\u001b[0m (26.76 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003e Non-trainable params: \u003c/span\u003e\u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e66,432\u003c/span\u003e (259.50 KB)\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m66,432\u001b[0m (259.50 KB)\n"]},"metadata":{},"output_type":"display_data"}],"source":["latent_dim = 128\n","\n","def build_generator():\n","    model = keras.Sequential(name=\"generator\")\n","\n","    # Input: Latent Vector\n","    # Mulai dengan dense layer yang cukup besar untuk di-reshape\n","    # Kita ingin mulai dari ukuran 8x8 dengan 512 filter\n","    model.add(layers.Dense(8 * 8 * 512, use_bias=False, input_shape=(latent_dim,)))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU(alpha=0.2))\n","\n","    # Reshape menjadi tensor 3D\n","    model.add(layers.Reshape((8, 8, 512)))\n","\n","    # Upsampling 1: 8x8 -\u003e 16x16\n","    model.add(layers.Conv2DTranspose(256, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU(alpha=0.2))\n","\n","    # Upsampling 2: 16x16 -\u003e 32x32\n","    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU(alpha=0.2))\n","\n","    # Upsampling 3: 32x32 -\u003e 64x64\n","    model.add(layers.Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU(alpha=0.2))\n","\n","    # Output Layer: 64x64 -\u003e 64x64x3 (RGB)\n","    # Aktivasi TANH penting agar output di range [-1, 1]\n","    model.add(layers.Conv2D(CHANNELS, (3, 3), padding='same', activation='tanh'))\n","\n","    return model\n","\n","def build_discriminator():\n","    model = keras.Sequential(name=\"discriminator\")\n","\n","    # Input: Gambar 64x64x3\n","    # Downsampling 1: 64 -\u003e 32\n","    model.add(layers.Conv2D(64, (4, 4), strides=(2, 2), padding='same', input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS)))\n","    model.add(layers.LeakyReLU(alpha=0.2))\n","    model.add(layers.Dropout(0.3))\n","\n","    # Downsampling 2: 32 -\u003e 16\n","    model.add(layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same'))\n","    model.add(layers.LeakyReLU(alpha=0.2))\n","    model.add(layers.Dropout(0.3))\n","\n","    # Downsampling 3: 16 -\u003e 8\n","    model.add(layers.Conv2D(256, (4, 4), strides=(2, 2), padding='same'))\n","    model.add(layers.LeakyReLU(alpha=0.2))\n","    model.add(layers.Dropout(0.3))\n","\n","    # Flatten dan Output\n","    model.add(layers.Flatten())\n","    model.add(layers.Dense(1)) # Output berupa logit (skor real/fake)\n","\n","    return model\n","\n","# Inisialisasi Model\n","generator = build_generator()\n","discriminator = build_discriminator()\n","\n","generator.summary()\n","# discriminator.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"miz1a5j1nOp5"},"outputs":[],"source":["class DCGAN(keras.Model):\n","    def __init__(self, discriminator, generator, latent_dim):\n","        super(DCGAN, self).__init__()\n","        self.discriminator = discriminator\n","        self.generator = generator\n","        self.latent_dim = latent_dim\n","\n","    def compile(self, d_optimizer, g_optimizer, loss_fn):\n","        super(DCGAN, self).compile()\n","        self.d_optimizer = d_optimizer\n","        self.g_optimizer = g_optimizer\n","        self.loss_fn = loss_fn\n","        # Metric trackers\n","        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n","        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n","\n","    @property\n","    def metrics(self):\n","        return [self.d_loss_metric, self.g_loss_metric]\n","\n","    def train_step(self, real_images):\n","        # 1. Sample random points in the latent space\n","        batch_size = tf.shape(real_images)[0]\n","        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n","\n","        # 2. Decode them to fake images\n","        generated_images = self.generator(random_latent_vectors)\n","\n","        # 3. Combine them with real images\n","        combined_images = tf.concat([generated_images, real_images], axis=0)\n","\n","        # 4. Assemble labels discriminating real from fake images\n","        # Label 1 untuk fake, 0 untuk real (atau sebaliknya, teknik ini menggunakan label smoothing)\n","        # Di sini kita pakai standar: 1=Real, 0=Fake.\n","        # Namun, karena kita concat [Fake, Real], maka labelnya: [0...0, 1...1]\n","        labels = tf.concat(\n","            [tf.zeros((batch_size, 1)), tf.ones((batch_size, 1))], axis=0\n","        )\n","\n","        # Tambahkan sedikit noise pada label (Label Smoothing) untuk stabilitas\n","        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n","\n","        # 5. Train the Discriminator\n","        with tf.GradientTape() as tape:\n","            predictions = self.discriminator(combined_images)\n","            d_loss = self.loss_fn(labels, predictions)\n","\n","        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n","        self.d_optimizer.apply_gradients(\n","            zip(grads, self.discriminator.trainable_weights)\n","        )\n","\n","        # 6. Sample random points in the latent space (lagi, untuk Generator)\n","        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n","\n","        # 7. Assemble labels that say \"all real images\" (We want to fool the discriminator)\n","        misleading_labels = tf.ones((batch_size, 1))\n","\n","        # 8. Train the Generator\n","        with tf.GradientTape() as tape:\n","            predictions = self.discriminator(self.generator(random_latent_vectors))\n","            g_loss = self.loss_fn(misleading_labels, predictions)\n","\n","        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n","        self.g_optimizer.apply_gradients(\n","            zip(grads, self.generator.trainable_weights)\n","        )\n","\n","        # Update metrics\n","        self.d_loss_metric.update_state(d_loss)\n","        self.g_loss_metric.update_state(g_loss)\n","\n","        return {\n","            \"d_loss\": self.d_loss_metric.result(),\n","            \"g_loss\": self.g_loss_metric.result(),\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BZ9zhvtjnW5t"},"outputs":[],"source":["class GANMonitor(keras.callbacks.Callback):\n","    def __init__(self, num_img=3, latent_dim=128):\n","        self.num_img = num_img\n","        self.latent_dim = latent_dim\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n","        generated_images = self.model.generator(random_latent_vectors)\n","        generated_images *= 127.5\n","        generated_images += 127.5\n","        generated_images.numpy()\n","\n","        fig = plt.figure(figsize=(10, 4))\n","        for i in range(self.num_img):\n","            plt.subplot(1, self.num_img, i+1)\n","            img = keras.utils.array_to_img(generated_images[i])\n","            plt.imshow(img)\n","            plt.axis('off')\n","\n","        plt.suptitle(f\"Epoch {epoch+1}\")\n","        plt.show() # Tampilkan inline di Colab\n","\n","        # Opsional: Simpan ke file\n","        # plt.savefig(f\"generated_lego_epoch_{epoch}.png\")\n","        # plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1qhcHnyUkYf6mMRt46Wt40uhMOroIk-e4"},"id":"vp3IWPtBnjpC","outputId":"ed175958-c5c8-4892-8a4c-5c8e268e218a"},"outputs":[],"source":["# Hyperparameters\n","EPOCHS = 100  # Tambahkan jumlah epoch untuk hasil lebih baik (misal: 100-200)\n","lr_generator = 0.0002\n","lr_discriminator = 0.0002\n","\n","# Inisialisasi DCGAN\n","dcgan = DCGAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n","\n","# Compile\n","dcgan.compile(\n","    d_optimizer=keras.optimizers.Adam(learning_rate=lr_discriminator, beta_1=0.5),\n","    g_optimizer=keras.optimizers.Adam(learning_rate=lr_generator, beta_1=0.5),\n","    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",")\n","\n","# Jalankan Training\n","print(\"Mulai Training...\")\n","dcgan.fit(\n","    train_dataset,\n","    epochs=EPOCHS,\n","    callbacks=[GANMonitor(num_img=3, latent_dim=latent_dim)]\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","provenance":[{"file_id":"1HerZtoylS402-6KbWndG3tsYShXBAXiM","timestamp":1764831277605}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}